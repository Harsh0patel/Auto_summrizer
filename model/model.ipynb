{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af3975bf",
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1753519286645,
     "user": {
      "displayName": "Harsh Patel",
      "userId": "10065254090426918737"
     },
     "user_tz": -330
    },
    "id": "af3975bf"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sentencepiece as spm\n",
    "import os\n",
    "import ast\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import Embedding, LSTM\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "GwiWpoZnp_6i",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 30256,
     "status": "ok",
     "timestamp": 1753519286629,
     "user": {
      "displayName": "Harsh Patel",
      "userId": "10065254090426918737"
     },
     "user_tz": -330
    },
    "id": "GwiWpoZnp_6i",
    "outputId": "c81a68d0-fe91-4984-eb6e-0427ca85e4d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1da132c9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 42,
     "status": "ok",
     "timestamp": 1753519288270,
     "user": {
      "displayName": "Harsh Patel",
      "userId": "10065254090426918737"
     },
     "user_tz": -330
    },
    "id": "1da132c9",
    "outputId": "62076105-7c38-4ea5-e37c-736e8c42f828"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13e7b478",
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1753519289441,
     "user": {
      "displayName": "Harsh Patel",
      "userId": "10065254090426918737"
     },
     "user_tz": -330
    },
    "id": "13e7b478"
   },
   "outputs": [],
   "source": [
    "# making tensors\n",
    "class Tensor_maker(Dataset):\n",
    "    def __init__(self, input, target):\n",
    "        self.input = input\n",
    "        self.target = target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = ast.literal_eval(self.input[idx])\n",
    "        target_ids = ast.literal_eval(self.target[idx])\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n",
    "            \"target_ids\": torch.tensor(target_ids, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# padding sequences\n",
    "def collate_fn(batch):\n",
    "    inputs = [i[\"input_ids\"] for i in batch]\n",
    "    targets = [i[\"target_ids\"] for i in batch]\n",
    "\n",
    "    inputs_with_padding = pad_sequence(inputs, batch_first=True, padding_value=0)\n",
    "    targets_with_padding = pad_sequence(targets, batch_first=True, padding_value=0)\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": inputs_with_padding,\n",
    "        \"target_ids\": targets_with_padding\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9db36ce4",
   "metadata": {
    "executionInfo": {
     "elapsed": 9479,
     "status": "ok",
     "timestamp": 1753519302129,
     "user": {
      "displayName": "Harsh Patel",
      "userId": "10065254090426918737"
     },
     "user_tz": -330
    },
    "id": "9db36ce4"
   },
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('/content/drive/MyDrive/Auto Summrizer/data/train_tokens.csv')\n",
    "df2 = pd.read_csv('/content/drive/MyDrive/Auto Summrizer/data/test_tokens.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac112652",
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1753519302144,
     "user": {
      "displayName": "Harsh Patel",
      "userId": "10065254090426918737"
     },
     "user_tz": -330
    },
    "id": "ac112652"
   },
   "outputs": [],
   "source": [
    "train_dataset = Tensor_maker(df1['text_tokens'], df1['abstract_tokens'])\n",
    "test_dataset = Tensor_maker(df2['text_tokens'], df2['abstract_tokens'])\n",
    "\n",
    "# Use smaller batch sizes to save memory\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89712271",
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1753519305775,
     "user": {
      "displayName": "Harsh Patel",
      "userId": "10065254090426918737"
     },
     "user_tz": -330
    },
    "id": "89712271"
   },
   "outputs": [],
   "source": [
    "# Encoder class\n",
    "class Encoderlstm(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_size, batch_first=True)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        x = self.embedding(input_ids)\n",
    "        outputs, (h, c) = self.lstm(x)\n",
    "        return outputs, (h, c)\n",
    "\n",
    "# Fixed Decoder class with memory-efficient attention\n",
    "class DecoderLSTM(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim + hidden_size, hidden_size=hidden_size, batch_first=True)\n",
    "        self.output_projection = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "        # Attention layers\n",
    "        self.attention = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.context_projection = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input_token, hidden_state, encoder_outputs):\n",
    "        # input_token: (batch_size, 1)\n",
    "        # hidden_state: tuple of (h, c) where h,c are (1, batch_size, hidden_size)\n",
    "        # encoder_outputs: (batch_size, src_len, hidden_size)\n",
    "\n",
    "        batch_size = input_token.size(0)\n",
    "        src_len = encoder_outputs.size(1)\n",
    "\n",
    "        # Embed input token\n",
    "        embedded = self.embedding(input_token)  # (batch_size, 1, embedding_dim)\n",
    "\n",
    "        # Calculate attention weights more efficiently\n",
    "        decoder_hidden = hidden_state[0].transpose(0, 1)  # (batch_size, 1, hidden_size)\n",
    "\n",
    "        # Expand decoder hidden to match encoder outputs length\n",
    "        decoder_hidden_expanded = decoder_hidden.expand(-1, src_len, -1)  # (batch_size, src_len, hidden_size)\n",
    "\n",
    "        # Concatenate and compute attention\n",
    "        combined = torch.cat([decoder_hidden_expanded, encoder_outputs], dim=2)  # (batch_size, src_len, hidden_size*2)\n",
    "        energy = torch.tanh(self.attention(combined))  # (batch_size, src_len, hidden_size)\n",
    "\n",
    "        # Attention weights\n",
    "        attention_weights = torch.softmax(energy.sum(dim=2), dim=1)  # (batch_size, src_len)\n",
    "\n",
    "        # Context vector\n",
    "        context = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs)  # (batch_size, 1, hidden_size)\n",
    "\n",
    "        # Apply context projection to reduce dimension if needed\n",
    "        context = self.context_projection(context)  # (batch_size, 1, hidden_size)\n",
    "\n",
    "        # Combine embedding and context\n",
    "        lstm_input = torch.cat([embedded, context], dim=2)  # (batch_size, 1, embedding_dim + hidden_size)\n",
    "\n",
    "        # LSTM forward pass\n",
    "        output, new_hidden = self.lstm(lstm_input, hidden_state)\n",
    "\n",
    "        # Project to vocabulary\n",
    "        logits = self.output_projection(output)  # (batch_size, 1, vocab_size)\n",
    "\n",
    "        return logits, new_hidden\n",
    "\n",
    "# Memory-efficient Trainer\n",
    "class Trainer:\n",
    "    def __init__(self, encoder, decoder, optimizer, loss_fn, device):\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "        self.device = device\n",
    "\n",
    "    def train(self, dataloader, epochs=5, teacher_forcing_ratio=0.5, checkpoint_path='/content/drive/MyDrive/Auto Summrizer/check_point/checkpoint.pth'):\n",
    "        self.encoder.train()\n",
    "        self.decoder.train()\n",
    "\n",
    "        # Create the directory if it doesn't exist\n",
    "        checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "\n",
    "        start_epoch = 0\n",
    "        if os.path.exists(checkpoint_path):\n",
    "            checkpoint = torch.load(checkpoint_path)\n",
    "            self.encoder.load_state_dict(checkpoint['encoder_state_dict'])\n",
    "            self.decoder.load_state_dict(checkpoint['decoder_state_dict'])\n",
    "            self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "            start_epoch = checkpoint['epoch']\n",
    "            print(f\"Resuming training from epoch {start_epoch}\")\n",
    "\n",
    "\n",
    "        for epoch in range(start_epoch, epochs):\n",
    "            total_loss = 0\n",
    "\n",
    "            for batch_idx, batch in enumerate(dataloader):\n",
    "                # Clear cache periodically\n",
    "                if batch_idx % 10 == 0:\n",
    "                    torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "                    gc.collect()\n",
    "\n",
    "                input_ids = batch[\"input_ids\"].to(self.device)\n",
    "                target_ids = batch[\"target_ids\"].to(self.device)\n",
    "\n",
    "                batch_size, tgt_len = target_ids.shape\n",
    "                vocab_size = self.decoder.output_projection.out_features\n",
    "\n",
    "                # Encode source\n",
    "                encoder_outputs, encoder_hidden = self.encoder(input_ids)\n",
    "\n",
    "                # Initialize decoder\n",
    "                decoder_hidden = encoder_hidden\n",
    "                decoder_input = target_ids[:, 0:1].to(self.device)  # (batch_size, 1) - start token\n",
    "\n",
    "                total_loss_batch = 0\n",
    "\n",
    "                # Decode step by step\n",
    "                for t in range(1, tgt_len):\n",
    "                    # Move decoder_hidden to the correct device if needed\n",
    "                    decoder_hidden = (decoder_hidden[0].to(self.device), decoder_hidden[1].to(self.device))\n",
    "\n",
    "                    # Forward pass\n",
    "                    decoder_output, decoder_hidden = self.decoder(\n",
    "                        decoder_input, decoder_hidden, encoder_outputs\n",
    "                    )\n",
    "\n",
    "                    # Get logits for current step\n",
    "                    logits = decoder_output.squeeze(1)  # (batch_size, vocab_size)\n",
    "\n",
    "                    # Calculate loss for current step\n",
    "                    targets = target_ids[:, t]  # (batch_size,)\n",
    "                    loss = self.loss_fn(logits, targets)\n",
    "                    total_loss_batch += loss\n",
    "\n",
    "                    # Teacher forcing\n",
    "                    if torch.rand(1).item() < teacher_forcing_ratio:\n",
    "                        decoder_input = target_ids[:, t:t+1].to(self.device)  # Use ground truth\n",
    "                    else:\n",
    "                        decoder_input = logits.argmax(dim=1, keepdim=True).to(self.device)  # Use prediction\n",
    "\n",
    "\n",
    "                # Average loss across sequence length\n",
    "                avg_loss = total_loss_batch / (tgt_len - 1)\n",
    "\n",
    "                # Backward pass\n",
    "                self.optimizer.zero_grad()\n",
    "                avg_loss.backward()\n",
    "\n",
    "                # Gradient clipping to prevent exploding gradients\n",
    "                torch.nn.utils.clip_grad_norm_(\n",
    "                    list(self.encoder.parameters()) + list(self.decoder.parameters()),\n",
    "                    max_norm=1.0\n",
    "                )\n",
    "\n",
    "                self.optimizer.step()\n",
    "\n",
    "                total_loss += avg_loss.item()\n",
    "\n",
    "                # Clear variables\n",
    "                del encoder_outputs, encoder_hidden, decoder_hidden\n",
    "                torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "                gc.collect()\n",
    "\n",
    "\n",
    "                if batch_idx % 50 == 0:\n",
    "                    print(f\"Epoch {epoch+1}, Batch {batch_idx}, Loss: {avg_loss.item():.4f}\")\n",
    "\n",
    "            avg_epoch_loss = total_loss / len(dataloader)\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}] Average Loss: {avg_epoch_loss:.4f}\")\n",
    "\n",
    "            # Save checkpoint\n",
    "            checkpoint = {\n",
    "                'epoch': epoch + 1,\n",
    "                'encoder_state_dict': self.encoder.state_dict(),\n",
    "                'decoder_state_dict': self.decoder.state_dict(),\n",
    "                'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            }\n",
    "            torch.save(checkpoint, checkpoint_path)\n",
    "            print(f\"Checkpoint saved to {checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7637fbba",
   "metadata": {
    "executionInfo": {
     "elapsed": 5293,
     "status": "ok",
     "timestamp": 1753519313503,
     "user": {
      "displayName": "Harsh Patel",
      "userId": "10065254090426918737"
     },
     "user_tz": -330
    },
    "id": "7637fbba"
   },
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "vocab_size = 2000\n",
    "embedding_dim = 256\n",
    "hidden_size = 512\n",
    "\n",
    "encoder = Encoderlstm(vocab_size, embedding_dim, hidden_size).to(device)\n",
    "decoder = DecoderLSTM(vocab_size, embedding_dim, hidden_size).to(device)\n",
    "\n",
    "# Use a smaller learning rate\n",
    "optimizer = torch.optim.Adam(\n",
    "    list(encoder.parameters()) + list(decoder.parameters()),\n",
    "    lr=0.005\n",
    ")\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "trainer = Trainer(encoder, decoder, optimizer, loss_fn, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf7be8bd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4843412,
     "status": "ok",
     "timestamp": 1753524185814,
     "user": {
      "displayName": "Harsh Patel",
      "userId": "10065254090426918737"
     },
     "user_tz": -330
    },
    "id": "cf7be8bd",
    "outputId": "3dea0214-e1ca-448d-aff2-fc7431495e8a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming training from epoch 13\n",
      "Epoch 14, Batch 0, Loss: 4.7753\n",
      "Epoch 14, Batch 50, Loss: 4.4761\n",
      "Epoch 14, Batch 100, Loss: 4.7878\n",
      "Epoch 14, Batch 150, Loss: 4.7350\n",
      "Epoch 14, Batch 200, Loss: 4.6240\n",
      "Epoch 14, Batch 250, Loss: 4.6578\n",
      "Epoch 14, Batch 300, Loss: 4.6504\n",
      "Epoch 14, Batch 350, Loss: 4.9117\n",
      "Epoch 14, Batch 400, Loss: 5.0231\n",
      "Epoch 14, Batch 450, Loss: 4.5736\n",
      "Epoch 14, Batch 500, Loss: 4.6292\n",
      "Epoch 14, Batch 550, Loss: 4.6962\n",
      "Epoch 14, Batch 600, Loss: 5.0547\n",
      "Epoch 14, Batch 650, Loss: 4.9083\n",
      "Epoch 14, Batch 700, Loss: 4.8355\n",
      "Epoch 14, Batch 750, Loss: 4.4749\n",
      "Epoch 14, Batch 800, Loss: 4.6903\n",
      "Epoch 14, Batch 850, Loss: 4.9506\n",
      "Epoch 14, Batch 900, Loss: 4.6603\n",
      "Epoch 14, Batch 950, Loss: 4.7854\n",
      "Epoch 14, Batch 1000, Loss: 4.8563\n",
      "Epoch 14, Batch 1050, Loss: 4.8646\n",
      "Epoch 14, Batch 1100, Loss: 4.7097\n",
      "Epoch 14, Batch 1150, Loss: 4.8195\n",
      "Epoch 14, Batch 1200, Loss: 4.5435\n",
      "Epoch 14, Batch 1250, Loss: 4.7404\n",
      "Epoch 14, Batch 1300, Loss: 4.7312\n",
      "Epoch 14, Batch 1350, Loss: 4.8688\n",
      "Epoch 14, Batch 1400, Loss: 5.0530\n",
      "Epoch 14, Batch 1450, Loss: 4.7731\n",
      "Epoch 14, Batch 1500, Loss: 4.7783\n",
      "Epoch 14, Batch 1550, Loss: 4.7643\n",
      "Epoch 14, Batch 1600, Loss: 4.7254\n",
      "Epoch [14/16] Average Loss: 4.7278\n",
      "Checkpoint saved to /content/drive/MyDrive/Auto Summrizer/check_point/checkpoint.pth\n",
      "Epoch 15, Batch 0, Loss: 4.6402\n",
      "Epoch 15, Batch 50, Loss: 4.7031\n",
      "Epoch 15, Batch 100, Loss: 4.7257\n",
      "Epoch 15, Batch 150, Loss: 4.8270\n",
      "Epoch 15, Batch 200, Loss: 4.6307\n",
      "Epoch 15, Batch 250, Loss: 4.8543\n",
      "Epoch 15, Batch 300, Loss: 4.6796\n",
      "Epoch 15, Batch 350, Loss: 4.5830\n",
      "Epoch 15, Batch 400, Loss: 4.6908\n",
      "Epoch 15, Batch 450, Loss: 4.6756\n",
      "Epoch 15, Batch 500, Loss: 4.8376\n",
      "Epoch 15, Batch 550, Loss: 4.5237\n",
      "Epoch 15, Batch 600, Loss: 4.6037\n",
      "Epoch 15, Batch 650, Loss: 4.6444\n",
      "Epoch 15, Batch 700, Loss: 4.8097\n",
      "Epoch 15, Batch 750, Loss: 4.5289\n",
      "Epoch 15, Batch 800, Loss: 4.8549\n",
      "Epoch 15, Batch 850, Loss: 4.6412\n",
      "Epoch 15, Batch 900, Loss: 4.7317\n",
      "Epoch 15, Batch 950, Loss: 4.9382\n",
      "Epoch 15, Batch 1000, Loss: 4.5365\n",
      "Epoch 15, Batch 1050, Loss: 4.7174\n",
      "Epoch 15, Batch 1100, Loss: 4.5435\n",
      "Epoch 15, Batch 1150, Loss: 4.7685\n",
      "Epoch 15, Batch 1200, Loss: 4.7057\n",
      "Epoch 15, Batch 1250, Loss: 4.6628\n",
      "Epoch 15, Batch 1300, Loss: 4.8435\n",
      "Epoch 15, Batch 1350, Loss: 4.8190\n",
      "Epoch 15, Batch 1400, Loss: 4.7354\n",
      "Epoch 15, Batch 1450, Loss: 4.7377\n",
      "Epoch 15, Batch 1500, Loss: 4.6375\n",
      "Epoch 15, Batch 1550, Loss: 4.6548\n",
      "Epoch 15, Batch 1600, Loss: 4.7596\n",
      "Epoch [15/16] Average Loss: 4.7168\n",
      "Checkpoint saved to /content/drive/MyDrive/Auto Summrizer/check_point/checkpoint.pth\n",
      "Epoch 16, Batch 0, Loss: 4.3233\n",
      "Epoch 16, Batch 50, Loss: 4.7391\n",
      "Epoch 16, Batch 100, Loss: 4.6574\n",
      "Epoch 16, Batch 150, Loss: 4.6092\n",
      "Epoch 16, Batch 200, Loss: 4.9032\n",
      "Epoch 16, Batch 250, Loss: 4.4771\n",
      "Epoch 16, Batch 300, Loss: 4.5207\n",
      "Epoch 16, Batch 350, Loss: 4.6991\n",
      "Epoch 16, Batch 400, Loss: 4.6203\n",
      "Epoch 16, Batch 450, Loss: 4.6668\n",
      "Epoch 16, Batch 500, Loss: 4.7847\n",
      "Epoch 16, Batch 550, Loss: 4.8653\n",
      "Epoch 16, Batch 600, Loss: 4.6901\n",
      "Epoch 16, Batch 650, Loss: 4.7493\n",
      "Epoch 16, Batch 700, Loss: 4.4616\n",
      "Epoch 16, Batch 750, Loss: 4.4786\n",
      "Epoch 16, Batch 800, Loss: 4.6605\n",
      "Epoch 16, Batch 850, Loss: 4.7071\n",
      "Epoch 16, Batch 900, Loss: 4.5130\n",
      "Epoch 16, Batch 950, Loss: 4.6945\n",
      "Epoch 16, Batch 1000, Loss: 4.8262\n",
      "Epoch 16, Batch 1050, Loss: 4.7168\n",
      "Epoch 16, Batch 1100, Loss: 4.6681\n",
      "Epoch 16, Batch 1150, Loss: 4.6817\n",
      "Epoch 16, Batch 1200, Loss: 4.6971\n",
      "Epoch 16, Batch 1250, Loss: 4.4338\n",
      "Epoch 16, Batch 1300, Loss: 4.7477\n",
      "Epoch 16, Batch 1350, Loss: 4.8331\n",
      "Epoch 16, Batch 1400, Loss: 4.8074\n",
      "Epoch 16, Batch 1450, Loss: 4.8296\n",
      "Epoch 16, Batch 1500, Loss: 4.5422\n",
      "Epoch 16, Batch 1550, Loss: 4.6363\n",
      "Epoch 16, Batch 1600, Loss: 4.7386\n",
      "Epoch [16/16] Average Loss: 4.6944\n",
      "Checkpoint saved to /content/drive/MyDrive/Auto Summrizer/check_point/checkpoint.pth\n"
     ]
    }
   ],
   "source": [
    "trainer.train(train_loader, epochs=16, teacher_forcing_ratio=0.5)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
