from model.summurizer import Encoderlstm, DecoderLSTM
import torch
import sentencepiece as spm

vocab_size = 2000
embedding_dim = 128
hidden_size = 256
encoder = Encoderlstm(vocab_size, embedding_dim, hidden_size)
decoder = DecoderLSTM(vocab_size, embedding_dim, hidden_size)
model = torch.load('check_point/checkpoint.pth')
encoder.load_state_dict(model['encoder_state_dict'])
decoder.load_state_dict(model['decoder_state_dict'])
sp = spm.SentencePieceProcessor('bpe.model')

def generate_summary(encoder, decoder, input_ids,  max_len=100, device='cpu'):
    encoder.eval()
    decoder.eval()

    with torch.no_grad():
        encoder_output, encoder_hidden = encoder(input_ids.to(device))

        # Start token ID
        start_token = sp.encode("sostok")[0]
        eos_token = sp.encode("eostok")[0]

        decoder_input = torch.tensor([[start_token]], device=device)
        decoder_hidden = encoder_hidden

        summary_ids = [start_token]

        for _ in range(max_len):
            output, decoder_hidden = decoder(decoder_input, decoder_hidden, encoder_output)
            logits = output[:, -1, :] if output.dim() == 3 else output  # Handle shape (1, 1, vocab) or (1, vocab)

            next_token = logits.argmax(1).item()
            summary_ids.append(next_token)

            # ðŸ›‘ Stop if <eostok>
            if next_token == eos_token:
                break
            decoder_input = torch.tensor([[next_token]], device=device)
    return sp.decode(summary_ids)  # remove <sostok> when returning

#example
import pandas as pd
df = pd.read_json('output\General_text_text_cleaned.jsonl', lines = True)
text = df.at[0, 'cleaned_text']
text = sp.encode(text)
text = torch.tensor(text).unsqueeze(0)
output = generate_summary(encoder, decoder, text)
print("output generated by model: ", output)
print("original output: ", df.at[0, 'cleaned_abstract'])