{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "887a1e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c346c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load small English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def preprocessing(text):\n",
    "    # 1. Remove URLs and promotional lines\n",
    "    text = re.sub(r\"http\\S+|www\\S+|watch.*?»\", \"\", text, flags=re.IGNORECASE)\n",
    "\n",
    "    # 2. Remove boilerplate/copyright notices\n",
    "    text = re.sub(r\"Copyright.*?reserved\\.\", \"\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\"This material.*?redistributed\", \"\", text, flags=re.IGNORECASE)\n",
    "\n",
    "    # 3. Normalize quotes/apostrophes\n",
    "    text = text.replace(\"’\", \"'\").replace(\"‘\", \"'\")\n",
    "    text = text.replace(\"“\", '\"').replace(\"”\", '\"')\n",
    "\n",
    "    # 4. Normalize whitespace\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    # 5. Sentence segmentation using spaCy\n",
    "    doc = nlp(text)\n",
    "    sentences = [sent.text.strip().capitalize() for sent in doc.sents if sent.text.strip()]\n",
    "\n",
    "    # Fix lowercase 'i' when it’s a pronoun\n",
    "    text = re.sub(r\"\\bi\\b\", \"I\", text)\n",
    "\n",
    "    # Remove trailing junk\n",
    "    text = re.sub(r\"E-mail to a friend\\s*\\.*\", \"\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
    "    text = re.sub(r\"\\.\\.+\", \".\", text)\n",
    "    text = re.sub(r\"\\b\\w\\b\", \"\", text)  # Remove single-letter tokens (can hurt if text has acronyms)\n",
    "    text = contractions.fix(text)\n",
    "    \n",
    "    # 6. Join cleaned sentences\n",
    "    clean_text = \" \".join(sentences)\n",
    "\n",
    "    return clean_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acd75d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Process a single file ---\n",
    "def process_file(filepath, output_dir):\n",
    "    try:\n",
    "        filename = os.path.basename(filepath)\n",
    "        print(f\"[INFO] Processing: {filename}\")\n",
    "\n",
    "        df = pd.read_json(filepath, lines = True)\n",
    "\n",
    "        if 'article' not in df.columns:\n",
    "            print(f\"[WARN] Skipping {filename}, no 'text' column.\")\n",
    "            return\n",
    "\n",
    "        tqdm.pandas(desc=f\"Cleaning {filename}\")\n",
    "        df['cleaned_text'] = df['article'].progress_apply(preprocessing)\n",
    "\n",
    "        if 'highlights' not in df.columns:\n",
    "            print(f\"[WARN] Skipping {filename}, no 'text' column.\")\n",
    "            return\n",
    "\n",
    "        tqdm.pandas(desc=f\"Cleaning {filename}\")\n",
    "        df['cleaned_abstract'] = df['highlights'].progress_apply(preprocessing)\n",
    "        \n",
    "        # Save cleaned output\n",
    "        output_path = os.path.join(output_dir, filename.replace('.jsonl', '_cleaned.jsonl'))\n",
    "        df.to_json(output_path, orient='records', lines=True)\n",
    "        print(f\"[DONE] Saved: {output_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to process {filepath}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57e042f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Processing: General_train_text_chunk_6.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning General_train_text_chunk_6.jsonl: 100%|██████████| 20000/20000 [1:03:12<00:00,  5.27it/s]\n",
      "Cleaning General_train_text_chunk_6.jsonl: 100%|██████████| 20000/20000 [05:09<00:00, 64.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] Saved: output\\General_train_text_chunk_6_cleaned.jsonl\n"
     ]
    }
   ],
   "source": [
    "process_file('data\\General_train_text_chunk_6.jsonl', 'output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "474ef8fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['article', 'highlights', 'id', 'cleaned_text', 'cleaned_abstract'], dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_json('output\\General_train_text_chunk_6_cleaned.jsonl', lines = True)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae02aaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns = ['article', 'highlights', 'id'], inplace = True)\n",
    "df.to_json('output\\General_train_text_chunk_6_cleaned.jsonl', orient = 'records', lines = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01b1115d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_json('output\\General_train_text_chunk_6_cleaned.jsonl', lines = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "071f651d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>cleaned_abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>By . Emily allen . Published: . 07:32 est, 9 a...</td>\n",
       "      <td>sostok Jason young and tyrell o'donnell stole ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>By . James salmon . Published: . 19:03 est, 7 ...</td>\n",
       "      <td>sostok New york state department of financial ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>By . Daily mail reporter . Published: . 05:40 ...</td>\n",
       "      <td>sostok Simon richardson, 44, suffered life-thr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>By . Daily mail reporter . Published: . 22:44 ...</td>\n",
       "      <td>sostok Abhay singh, 11, and sister amanat, 9, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>By . Daily mail reporter . Published: . 10:34 ...</td>\n",
       "      <td>sostok Lawyers for megaupload boss kim dotcom ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19995</th>\n",
       "      <td>By . Lauren paxman . Updated: . 08:30 est, 7 m...</td>\n",
       "      <td>sostok Link was discovered after man suffering...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19996</th>\n",
       "      <td>By . Rob cooper . Published: . 10:34 est, 23 m...</td>\n",
       "      <td>sostok Mark bridger, 47, said he was suffering...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19997</th>\n",
       "      <td>By . Jaymi mccann . Published: . 05:34 est, 23...</td>\n",
       "      <td>sostok Dale pipe, 20, asked 'why so serious?' ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19998</th>\n",
       "      <td>By . Martha de lacey . Published: . 10:31 est,...</td>\n",
       "      <td>sostok Shoes being auctioned online by pfc auc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19999</th>\n",
       "      <td>By . Sean poulter . Published: . 20:05 est, 29...</td>\n",
       "      <td>sostok For public trust the church only ranked...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            cleaned_text  \\\n",
       "0      By . Emily allen . Published: . 07:32 est, 9 a...   \n",
       "1      By . James salmon . Published: . 19:03 est, 7 ...   \n",
       "2      By . Daily mail reporter . Published: . 05:40 ...   \n",
       "3      By . Daily mail reporter . Published: . 22:44 ...   \n",
       "4      By . Daily mail reporter . Published: . 10:34 ...   \n",
       "...                                                  ...   \n",
       "19995  By . Lauren paxman . Updated: . 08:30 est, 7 m...   \n",
       "19996  By . Rob cooper . Published: . 10:34 est, 23 m...   \n",
       "19997  By . Jaymi mccann . Published: . 05:34 est, 23...   \n",
       "19998  By . Martha de lacey . Published: . 10:31 est,...   \n",
       "19999  By . Sean poulter . Published: . 20:05 est, 29...   \n",
       "\n",
       "                                        cleaned_abstract  \n",
       "0      sostok Jason young and tyrell o'donnell stole ...  \n",
       "1      sostok New york state department of financial ...  \n",
       "2      sostok Simon richardson, 44, suffered life-thr...  \n",
       "3      sostok Abhay singh, 11, and sister amanat, 9, ...  \n",
       "4      sostok Lawyers for megaupload boss kim dotcom ...  \n",
       "...                                                  ...  \n",
       "19995  sostok Link was discovered after man suffering...  \n",
       "19996  sostok Mark bridger, 47, said he was suffering...  \n",
       "19997  sostok Dale pipe, 20, asked 'why so serious?' ...  \n",
       "19998  sostok Shoes being auctioned online by pfc auc...  \n",
       "19999  sostok For public trust the church only ranked...  \n",
       "\n",
       "[20000 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2['cleaned_abstract'] = df2['cleaned_abstract'].apply(lambda x: 'sostok ' + x + ' eostok')\n",
    "df2\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff90d2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.to_json('output\\General_train_text_chunk_6_cleaned.jsonl', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c070ff2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load spaCy English model\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# def clean_text_regex(text):\n",
    "#     text = re.sub(r'xmath\\d+', '', text)  # Remove math symbols like xmath123\n",
    "#     text = re.sub(r'\\\\[a-zA-Z]+', '', text)  # Remove LaTeX commands like \\alpha, \\beta, \\sum, etc.\n",
    "#     text = re.sub(r'xcite', '', text)  # Remove citation placeholders\n",
    "#     text = re.sub(r'\\[.*?\\]', '', text)  # Remove brackets and references\n",
    "    \n",
    "#     # Remove math equations - LaTeX style (between $ $ or $$ $$)\n",
    "#     text = re.sub(r'\\$\\$.*?\\$\\$', '', text, flags=re.DOTALL)  # Display math\n",
    "#     text = re.sub(r'\\$.*?\\$', '', text)  # Inline math\n",
    "    \n",
    "#     # Remove math equations - parentheses style\n",
    "#     text = re.sub(r'\\\\\\(.*?\\\\\\)', '', text, flags=re.DOTALL)  # Inline math\n",
    "#     text = re.sub(r'\\\\\\[.*?\\\\\\]', '', text, flags=re.DOTALL)  # Display math\n",
    "    \n",
    "#     # Remove equation environments\n",
    "#     text = re.sub(r'\\\\begin\\{equation\\*?\\}.*?\\\\end\\{equation\\*?\\}', '', text, flags=re.DOTALL)\n",
    "#     text = re.sub(r'\\\\begin\\{align\\*?\\}.*?\\\\end\\{align\\*?\\}', '', text, flags=re.DOTALL)\n",
    "#     text = re.sub(r'\\\\begin\\{eqnarray\\*?\\}.*?\\\\end\\{eqnarray\\*?\\}', '', text, flags=re.DOTALL)\n",
    "#     text = re.sub(r'\\\\begin\\{gather\\*?\\}.*?\\\\end\\{gather\\*?\\}', '', text, flags=re.DOTALL)\n",
    "#     text = re.sub(r'\\\\begin\\{multline\\*?\\}.*?\\\\end\\{multline\\*?\\}', '', text, flags=re.DOTALL)\n",
    "#     text = re.sub(r'\\\\begin\\{split\\}.*?\\\\end\\{split\\}', '', text, flags=re.DOTALL)\n",
    "    \n",
    "#     # Remove mathematical operators and symbols\n",
    "#     text = re.sub(r'[+\\-*/=<>≤≥≠≈∞∑∏∫∂∇∆√∈∉⊂⊃∪∩∧∨¬∀∃]', '', text)\n",
    "    \n",
    "#     # Remove fractions pattern like a/b where a and b are numbers or variables\n",
    "#     text = re.sub(r'\\b\\w+/\\w+\\b', '', text)\n",
    "    \n",
    "#     # Remove superscripts and subscripts (basic patterns)\n",
    "#     text = re.sub(r'\\^[{\\w}]+', '', text)  # Remove ^{something} or ^word\n",
    "#     text = re.sub(r'_[{\\w}]+', '', text)   # Remove _{something} or _word\n",
    "    \n",
    "#     # Remove curly braces and their contents (often used in math)\n",
    "#     text = re.sub(r'\\{[^}]*\\}', '', text)\n",
    "    \n",
    "#     # Remove common math function names\n",
    "#     math_functions = [\n",
    "#         'sin', 'cos', 'tan', 'log', 'ln', 'exp', 'sqrt', 'abs', 'max', 'min',\n",
    "#         'lim', 'sup', 'inf', 'det', 'tr', 'rank', 'dim', 'ker', 'im',\n",
    "#         'gcd', 'lcm', 'mod', 'deg', 'arg'\n",
    "#     ]\n",
    "#     for func in math_functions:\n",
    "#         text = re.sub(rf'\\b{func}\\b', '', text, flags=re.IGNORECASE)\n",
    "    \n",
    "#     # Remove Greek letters (common in math)\n",
    "#     greek_letters = [\n",
    "#         'alpha', 'beta', 'gamma', 'delta', 'epsilon', 'zeta', 'eta', 'theta',\n",
    "#         'iota', 'kappa', 'lambda', 'mu', 'nu', 'xi', 'omicron', 'pi', 'rho',\n",
    "#         'sigma', 'tau', 'upsilon', 'phi', 'chi', 'psi', 'omega'\n",
    "#     ]\n",
    "#     for letter in greek_letters:\n",
    "#         text = re.sub(rf'\\b{letter}\\b', '', text, flags=re.IGNORECASE)\n",
    "    \n",
    "#     # Remove mathematical expressions with variables (like x + y = z)\n",
    "#     text = re.sub(r'\\b[a-zA-Z]\\s*[+\\-*/=]\\s*[a-zA-Z0-9]+', '', text)\n",
    "    \n",
    "#     # Remove sequences of mathematical symbols\n",
    "#     text = re.sub(r'[∀∃∈∉⊂⊃∪∩∧∨¬→↔≡⊕⊗∅ℕℤℚℝℂ]{2,}', '', text)\n",
    "    \n",
    "#     # Remove parentheses with only mathematical content\n",
    "#     text = re.sub(r'\\([^a-zA-Z]*\\)', '', text)\n",
    "    \n",
    "#     # Remove standalone numbers (original rule)\n",
    "#     text = re.sub(r'\\b\\d+\\b', '', text)\n",
    "    \n",
    "#     # Remove floating point numbers\n",
    "#     text = re.sub(r'\\b\\d+\\.\\d+\\b', '', text)\n",
    "    \n",
    "#     # Remove scientific notation\n",
    "#     text = re.sub(r'\\b\\d+\\.?\\d*[eE][+\\-]?\\d+\\b', '', text)\n",
    "    \n",
    "#     # Clean up multiple spaces and normalize whitespace\n",
    "#     text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "#     # Remove extra punctuation that might be left over\n",
    "#     text = re.sub(r'[,;:]{2,}', '', text)\n",
    "#     text = re.sub(r'\\.{2,}', '.', text)\n",
    "    \n",
    "#     return text.strip()\n",
    "\n",
    "\n",
    "# def basic_clean(text):\n",
    "#     text = text.lower()\n",
    "#     text = re.sub(r'\\n+', ' ', text)                   # remove line breaks\n",
    "#     text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)  # remove URLs\n",
    "#     text = re.sub(r'\\S+@\\S+', '', text)                # remove emails\n",
    "#     text = re.sub(r'\\d{10,}', '', text)                # remove long numbers (like phone numbers)\n",
    "#     text = re.sub(r'[^a-zA-Z\\s]', '', text)            # remove punctuation and digits\n",
    "#     text = re.sub(r'\\s+', ' ', text).strip()           # collapse multiple spaces\n",
    "#     return text\n",
    "\n",
    "# def clean_with_textacy(text):\n",
    "#     doc = nlp(text)\n",
    "#     tokens = [\n",
    "#         token.lemma_\n",
    "#         for token in doc\n",
    "#         if not token.is_stop and token.is_alpha and token.pos_ in {\"NOUN\", \"VERB\", \"ADJ\", \"ADV\"}\n",
    "#     ]\n",
    "#     return \" \".join(tokens)\n",
    "\n",
    "# def preprocess_text(text):\n",
    "#     step0 = clean_text_regex(text)\n",
    "#     step1 = basic_clean(step0)\n",
    "#     step2 = clean_with_textacy(step1)\n",
    "#     return step2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22238c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example usage\n",
    "# raw_text = \"Additive models provide flexibility, better interpretability, and avoid the curse of dimensionality!\"\n",
    "# cleaned_text = preprocess_text(raw_text)\n",
    "# print(\"before cleaning: \", raw_text)\n",
    "# print(\"after cleaning: \",cleaned_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
